_target_: src.models.rwkv_wiki_module.RWKVWikiLitModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 8e-4
  betas: [0.9, 0.99]
  eps: 1e-8

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
  _partial_: true
  lr_lambda:
    _target_: src.models.components.warmup_exp_decay_lr_lambda.WarmupExpDecayLRLambda
    base_lr: ${...optimizer.lr}
    init_lr: 1e-7
    max_lr: 8e-4 # If warmup_steps is 0, this value is used as initial learning rate.
    final_lr: 1e-5
    warmup_steps: 0 # If learning from pre-trained model, set other than 0.
    max_steps: 10000
scheduler_update_frequecy: 1 # steps

net:
  _target_: src.models.components.rwkv_lang.RWKVLang
  dim: 512
  vocab_size: 32768
  model:
    _target_: src.models.components.rwkv_lang.RWKV
    dim: ${..dim}
    depth: 6
    hidden_dim_factor: 4

monitoring_text_dataset:
  _target_: src.data.components.text_dataset.SPTokenizingTextDataset
  data_dirs:
    - ${paths.root_dir}/monitoring_texts
  sp_processor:
    _target_: sentencepiece.SentencePieceProcessor
    model_file: ${paths.data_dir}/WikiCorpusJa/sp_tokenizer_32k.model
  max_len: 1024
  ignoring_chars: "\n"
  replacement: " "
  list_file_recusively: true

monitoring_interval: 100
monitoring_n_samples: 10
num_generating_tokens: 512
do_init_weights: true # If learning from pre-trained model, set false.
